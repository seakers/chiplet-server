{
    "model": "gpt",
    "model_instance": "gpt-j",
    "batch_size": 128,
    "workload_trace": [
        {
            "kernel": "Attention",
            "input": {
                "batch-size": 1,
                "beam-size": 4,
                "seq-len": 0,
                "hidden-dim": 4096,
                "kv_cache": 128,
                "uncached": true
            },
            "output": {
                "batch-size": 1,
                "beam-size": 4,
                "seq-len": 0,
                "hidden-dim": 4096
            },
            "layer": {
                "hidden-dim": 4096,
                "output-dim": 4096,
                "num-heads": 16,
                "params": 16777216
            }
        },
        {
            "kernel": "LayerNorm",
            "input": {
                "batch-size": 1,
                "beam-size": 4,
                "seq-len": 1,
                "hidden-dim": 4096
            },
            "output": {
                "batch-size": 1,
                "beam-size": 4,
                "seq-len": 1,
                "hidden-dim": 4096
            },
            "layer": {
                "weights": 8192,
                "params": 8192
            }
        },
        {
            "kernel": "Linear",
            "input": {
                "batch-size": 4,
                "hidden-dim": 4096,
                "params": 16384,
                "non-zeros": 16384
            },
            "output": {
                "batch-size": 4,
                "output-dim": 16384
            },
            "layer": {
                "hidden-dim": 4096,
                "output-dim": 16384,
                "params": 67125248
            }
        },
        {
            "kernel": "GeLU",
            "input": {
                "elements": 65536
            },
            "output": {
                "elements": 65536
            },
            "layer": {}
        },
        {
            "kernel": "Linear",
            "input": {
                "batch-size": 4,
                "hidden-dim": 16384,
                "params": 65536,
                "non-zeros": 65534
            },
            "output": {
                "batch-size": 4,
                "output-dim": 4096
            },
            "layer": {
                "hidden-dim": 16384,
                "output-dim": 4096,
                "params": 67112960
            }
        },
        {
            "kernel": "LayerNorm",
            "input": {
                "batch-size": 1,
                "beam-size": 4,
                "seq-len": 1,
                "hidden-dim": 4096
            },
            "output": {
                "batch-size": 1,
                "beam-size": 4,
                "seq-len": 1,
                "hidden-dim": 4096
            },
            "layer": {
                "weights": 8192,
                "params": 8192
            }
        }
    ]
}